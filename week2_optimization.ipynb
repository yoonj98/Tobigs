{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tobigs 2주차 Optimization 과제\n",
    "#### 15기 이윤정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Gradient Descent 구현하기\n",
    " 1)\"...\"표시되어 있는 빈 칸을 채워주세요  \n",
    " 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.25 : 0.75로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때, scaler는 X_train에 fit되었으므로 X_test를 scaling할 때에는 transform만 해주면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44929453, 0.99802964, 0.31350775])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "random_parameters = parameters.copy()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "## $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z += X[i] * parameters[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내적 함수 이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p = 1/(1+e^{-\\beta *X})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = dot_product(X, parameters)\n",
    "    p = 1 / (1 + np.exp(-z)) \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid function로 불리며, 수식 상의 $\\beta*X$는 Z로 표현되기도 한다.  \n",
    "sigmoid function은 0에서 1 사이의 값만은 output으로 갖기 때문에 binary classification 혹은 확률 P를 나타내는 데 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8383903700275378"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "<br>\n",
    "선형 회귀의 목적함수\n",
    "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
    "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
    "  \n",
    "로지스틱 회귀의 목적함수\n",
    "## $l(p) =-\\sum \\left \\{ y_{i}logp + (1-y_{i})log(1-p)  \\right \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE를 목적함수 (=비용함수)로 사용하는 linear regression과 달리 logistic regression은 목적함수로 MSE를 사용하지 않는다. MSE를 목적함수로 Gradiant Descent 사용 시 Convex function이 아니게 되어 찾고자하는 최솟값이 아닌 다른 최솟값에 도달할 수 있기 때문이다.  \n",
    "<br>\n",
    "따라서, MSE를 대체할 수 있는 목적함수를 찾은 결과, 다음 조건을 만족한다.  \n",
    "       $if, y = 1\\rightarrow cost(p,y) = −log(p)$  \n",
    "       $if, y = 0\\rightarrow cost(p,y) = −log(1-p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X, parameters)\n",
    "    loss = (y*np.log(p)+(1-y)*np.log(1-p))\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_i(X, y, parameters):\n",
    "    y_hat = np.dot(X, parameters.T)\n",
    "    loss = np.square(np.subtract(y,y_hat))/2  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X, y, parameters)\n",
    "    loss = loss / n #loss 평균값으로 계산\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.150046123401088"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)=-\\Sigma(y_i - \\theta^{T}X_i)X_{ij}$  \n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)=-\\sum \\left ( y_{i}-p_{i} \\right )x_{ij}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gradient_ij(X, y, parameters, j, model):\n",
    "    if model == 'linear':\n",
    "        y_hat = np.dot(X, parameters.T)\n",
    "        gradient = (y - y_hat) * X[j]\n",
    "    else:\n",
    "        p = logistic(X, parameters)\n",
    "        gradient = -(y-p)*X[j]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08094620883776002"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient\n",
    "배치 경사 하강법이란 매 경사 하강법 step에서 전체 훈련 세트 X에 대해 계산하는 기법으로, 특성 수에 민감하지 않지만 훈련 세트가 클수록 속도가 느려지는 단점이 존재한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for _ in range(len(parameters))]\n",
    "    \n",
    "    for i in range(len(X_set)):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij(X, y, parameters, j, model)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44.84195179561546, 18.204413828877563, 44.49454953197766]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch\n",
    "미니 배치라 불리우는 임의의 작은 샘플 세트에 대해 그레디언트를 계산하는 기법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
    "- batch_idx 함수는 parameter로 입력받은 batch_size를 기준으로 데이터를 인덱싱하여 미니배치로 만든다음 그레디언트를 계산하는 함수이다. \n",
    "- 미니배치를 어느 정도 크게 하면 Stochastic Gradient Descent(SGD)보다 최솟값에 더 가까이 도달할 수 있다고 한다. 다만, 해당 최솟값이 local minimum일 경우 빠져나오는 데 더 힘들다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "기울기를 갱신하는 코드를 작성해주세요  \n",
    "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)):\n",
    "        gradients[i] *= learning_rate / n\n",
    "    \n",
    "    parameters -= gradients\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44630507, 0.99681601, 0.31054145])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(parameters, gradients1, 0.01, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
    "\n",
    "- learning_rate: 학습률  \n",
    "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
    "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
    "- epoch: 한 반복에서 훈련 세트의 샘플 수만큼 반복되는 데, 이때 각 반복을 의미\n",
    "- num_epoch: 최대반복횟수 \n",
    "<br>\n",
    "\n",
    "BGD: 학습 한 번에 모든 데이터셋에 대해 기울기를 구한다.    \n",
    "SGD: 학습 한 번에 임의의 데이터에 대해서만 기울기를 구한다.   \n",
    "MGD: 학습 한 번에 데이터셋의 일부에 대해서만 기울기를 구한다.    \n",
    "\n",
    "<br>\n",
    "batch_size에 따른 경사하강법의 종류를 적어주세요. <br>      \n",
    "batch_size=1 -> SGD  <br>\n",
    "batch_size=k -> MGD  <br>\n",
    "batch_size=whole -> BGD       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.random.rand(N)\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    loss = 999\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        if stopper:\n",
    "            break\n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch, y_batch, parameters, model)\n",
    "            parameters = step(parameters, gradients, learning_rate, batch_size)\n",
    "            new_loss = batch_loss(X_batch, y_batch, parameters, loss_function, batch_size)\n",
    "            \n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                break\n",
    "            loss = new_loss\n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.7520044856529305  params: [0.30123424 0.34597166 0.05291009]  gradients: [0.029882521203362333, -0.002703105180106588, 0.01702461061568043]\n",
      "epoch: 100  loss: 0.4436696763743217  params: [-0.84224023  0.94325681 -0.86469777]  gradients: [0.0035838399905870357, -0.005986841816376762, 0.0060625768360992845]\n",
      "epoch: 200  loss: 0.38757044213019026  params: [-1.04713159  1.45193026 -1.36703742]  gradients: [0.0012757608093173208, -0.004315162997860895, 0.004214769856983426]\n",
      "epoch: 300  loss: 0.3590698432566867  params: [-1.14997801  1.82631292 -1.73136549]  gradients: [0.0008688089846448446, -0.003265496379052847, 0.003166353113201251]\n",
      "epoch: 400  loss: 0.3421216783194798  params: [-1.22782806  2.11645641 -2.01184028]  gradients: [0.0007048696881395603, -0.0025893750776444257, 0.00249537068066953]\n",
      "epoch: 500  loss: 0.3311366726167629  params: [-1.29254201  2.3505288  -2.23682414]  gradients: [0.0005959075465002316, -0.00212313952024077, 0.0020354161476134553]\n",
      "epoch: 600  loss: 0.3235875440617085  params: [-1.34773374  2.54485808 -2.42271187]  gradients: [0.0005120343800527139, -0.0017833939133067161, 0.0017022126602904439]\n",
      "epoch: 700  loss: 0.3181756072444281  params: [-1.39542586  2.70960601 -2.57966397]  gradients: [0.00044483958933972995, -0.0015251611653741613, 0.0014503043894532408]\n",
      "epoch: 800  loss: 0.31417030860012407  params: [-1.43705108  2.85149904 -2.71437378]  gradients: [0.0003899996304502267, -0.0013224191013785445, 0.0012534766644848622]\n",
      "epoch: 900  loss: 0.311131731145024  params: [-1.4736897   2.97521668 -2.83147622]  gradients: [0.0003446061377642684, -0.0011591509657375187, 0.0010956562721360908]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.50586826,  3.08312175, -2.93334578])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train, batch_size = 150)\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.2808898919636226  params: [-0.86205675  1.09774093 -1.22428727]  gradients: [0.0253377851180744, 0.013788846896038971, 0.017827431026530904]\n",
      "epoch: 100  loss: 0.07736671174749421  params: [-1.93032571  4.17501957 -4.0676912 ]  gradients: [0.007538543134334528, 0.004102482384080787, 0.005304049155899163]\n",
      "epoch: 200  loss: 0.0773626655213781  params: [-1.9303681   4.1751431  -4.06780374]  gradients: [0.007538159521789448, 0.004102273621766841, 0.00530377924966401]\n",
      "epoch: 300  loss: 0.07736266518361716  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489767122, 0.004102273604340261, 0.005303779227133398]\n",
      "epoch: 400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.9303681 ,  4.17514311, -4.06780375])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, batch_size = 1)\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.14383432238500782  params: [0.59739362 0.57735427 0.73717589]  gradients: [0.005489968851400484, 0.004943158059939121, 0.006247955084048144]\n",
      "epoch: 100  loss: 0.02870588108827609  params: [-1.03683263  1.39765397 -1.31389325]  gradients: [0.00098480864057536, 0.001176464837135871, 0.001843736972057432]\n",
      "epoch: 200  loss: 0.02329850729256944  params: [-1.22588689  2.1061324  -2.00238953]  gradients: [0.0008437446706171071, 0.0011085068945596462, 0.0015882553149786734]\n",
      "epoch: 300  loss: 0.020641574995106434  params: [-1.35083953  2.55248698 -2.43053506]  gradients: [0.0007865029823557018, 0.0010800478274526773, 0.0014634323743798368]\n",
      "epoch: 400  loss: 0.0190389937627049  params: [-1.44294672  2.86792139 -2.73050021]  gradients: [0.000755548802372873, 0.0010632824493877703, 0.001387953385386587]\n",
      "epoch: 500  loss: 0.01796799758669633  params: [-1.51360561  3.10513544 -2.95467367]  gradients: [0.0007370613621296939, 0.0010527169393681442, 0.0013377772056523893]\n",
      "epoch: 600  loss: 0.017204481963900335  params: [-1.56940921  3.29047148 -3.12899406]  gradients: [0.0007252159634678162, 0.0010456888632886451, 0.0013022642270262078]\n",
      "epoch: 700  loss: 0.016635524311103777  params: [-1.61443717  3.43903435 -3.26821296]  gradients: [0.0007172095523869772, 0.0010408001842210218, 0.0012759888600441414]\n",
      "epoch: 800  loss: 0.016197761234306034  params: [-1.65135934  3.56031438 -3.38153223]  gradients: [0.0007115676605992192, 0.0010372742085441415, 0.0012559025954362944]\n",
      "epoch: 900  loss: 0.01585275185947297  params: [-1.68200963  3.66067422 -3.47508225]  gradients: [0.0007074577572266802, 0.0010346551387438033, 0.0012401619732811]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.70746314,  3.74382152, -3.55243763])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train, batch_size = 72)\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2. learning_rate = 0.001, num_epoch = 1000, tolerance = 0.00001\n",
    "learning_rate 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.1221972148447468  params: [0.89510853 0.96789474 0.59016699]  gradients: [0.00036624843753308426, 0.00011987346630492903, 0.00031064867168187067]\n",
      "epoch: 100  loss: 1.0979459786598849  params: [0.85867315 0.95600553 0.55924232]  gradients: [0.00036247884030796314, 0.00011791213256063098, 0.00030785166099342123]\n",
      "epoch: 200  loss: 1.0741956810519968  params: [0.8226202  0.94431802 0.52860377]  gradients: [0.0003586008136834171, 0.00011584002504887193, 0.00030492706481683124]\n",
      "epoch: 300  loss: 1.0509590624975986  params: [0.78696054 0.9328434  0.49826421]  gradients: [0.00035461396963380894, 0.0001136554850063232, 0.00030187298263151973]\n",
      "epoch: 400  loss: 1.0282483748025306  params: [0.75170507 0.92159296 0.46823666]  gradients: [0.0003505184085954441, 0.00011135730934037094, 0.0002986880338405919]\n",
      "epoch: 500  loss: 1.0060752696460407  params: [0.71686461 0.9105781  0.43853425]  gradients: [0.0003463147648762967, 0.00010894481578935033, 0.0002953714324199555]\n",
      "epoch: 600  loss: 0.9844506827377905  params: [0.68244994 0.89981028 0.40917015]  gradients: [0.00034200424973048914, 0.00010641790842468545, 0.00029192306234957486]\n",
      "epoch: 700  loss: 0.963384715105322  params: [0.64847163 0.8893009  0.38015752]  gradients: [0.00033758869083732994, 0.00010377714185969435, 0.00028834355204403753]\n",
      "epoch: 800  loss: 0.9428865133380403  params: [0.61494008 0.8790613  0.35150941]  gradients: [0.0003330705668097278, 0.0001010237822497552, 0.00028463434565971306]\n",
      "epoch: 900  loss: 0.9229641509120885  params: [0.5818654  0.86910264 0.32323867]  gradients: [0.0003284530352743032, 9.815986291943604e-05, 0.00028079776883917207]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.54958107, 0.85953103, 0.29563473])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd2 = gradient_descent(X_train, y_train, learning_rate = 0.001, batch_size = 150)\n",
    "new_param_bgd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.2095137810389445  params: [-0.00902156  0.44470522  0.88434469]  gradients: [0.0007019208203959662, 0.0003819859818243497, 0.0004938649906999472]\n",
      "epoch: 100  loss: 0.2947436613005141  params: [-0.96007212  1.05094432 -0.97004271]  gradients: [0.0002553645037082569, 0.000138969607166032, 0.00017967210059082]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.03714505,  1.36014727, -1.27605922])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd2 = gradient_descent(X_train, y_train, learning_rate = 0.001, batch_size = 1)\n",
    "new_param_sgd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.10018663975609157  params: [0.23446312 0.90645791 0.04659103]  gradients: [4.621131685822704e-05, 4.346017339170985e-05, 5.443463028858351e-05]\n",
      "epoch: 100  loss: 0.09426991471445206  params: [ 0.18000721  0.8897524  -0.00432069]  gradients: [4.450274237108169e-05, 4.2071698392531995e-05, 5.286957719901172e-05]\n",
      "epoch: 200  loss: 0.0888328308827762  params: [ 0.12768754  0.87481411 -0.05299949]  gradients: [4.279286690359129e-05, 4.0654204544346346e-05, 5.127795423872298e-05]\n",
      "epoch: 300  loss: 0.0838579194418641  params: [ 0.07751213  0.86165161 -0.09943744]  gradients: [4.1097534665583335e-05, 3.92234682002808e-05, 4.967747028285346e-05]\n",
      "epoch: 400  loss: 0.07932296905061582  params: [ 0.02947143  0.85025211 -0.14365159]  gradients: [3.9431414237490456e-05, 3.7795069876597916e-05, 4.8085275024363395e-05]\n",
      "epoch: 500  loss: 0.0752020698286007  params: [-0.01646073  0.84058247 -0.1856827 ]  gradients: [3.780741467907447e-05, 3.638359914323402e-05, 4.6517125935181414e-05]\n",
      "epoch: 600  loss: 0.07146676136118604  params: [-0.06032545  0.83259131 -0.22559276]  gradients: [3.623629236649842e-05, 3.5002017310080725e-05, 4.498675447619353e-05]\n",
      "epoch: 700  loss: 0.06808717197572645  params: [-0.10217657  0.82621182 -0.26346161]  gradients: [3.472646347261288e-05, 3.366123000127483e-05, 4.350547442305581e-05]\n",
      "epoch: 800  loss: 0.06503305566033327  params: [-0.14207817  0.82136513 -0.29938299]  gradients: [3.328400548408392e-05, 3.236988104991995e-05, 4.208203179601562e-05]\n",
      "epoch: 900  loss: 0.06227466344091584  params: [-0.18010207  0.81796367 -0.33346048]  gradients: [3.1912809207335554e-05, 3.113434474154218e-05, 4.0722661851196354e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.2159718 ,  0.81592855, -0.36548845])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd2 = gradient_descent(X_train, y_train, learning_rate = 0.001, batch_size = 72)\n",
    "new_param_mgd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3. learning_rate = 0.1, num_epoch = 10000, tolerance = 0.00001\n",
    "num_epoch 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.0255316007038509  params: [0.94710388 0.67718815 0.24961988]  gradients: [0.041692801871354344, 0.004096585311898844, 0.023368816084854485]\n",
      "epoch: 100  loss: 0.4422640281513257  params: [-0.77598786  0.98382936 -0.89596869]  gradients: [0.005032166222939261, -0.005551919792339754, 0.005999635877510051]\n",
      "epoch: 200  loss: 0.38589832570733207  params: [-1.04055032  1.4731046  -1.38771709]  gradients: [0.0014641440165372797, -0.004225300937278978, 0.00413042585134066]\n",
      "epoch: 300  loss: 0.3581048338397383  params: [-1.15168446  1.841427   -1.74605652]  gradients: [0.0008958214047224435, -0.003223338941969022, 0.003123922536388928]\n",
      "epoch: 400  loss: 0.34151594804793756  params: [-1.23057389  2.12828141 -2.02324036]  gradients: [0.000706192021224707, -0.0025633773225271998, 0.002469546627442465]\n",
      "epoch: 500  loss: 0.3307298200944027  params: [-1.29514955  2.3601943  -2.24608964]  gradients: [0.0005931316865623776, -0.0021050855540694084, 0.0020176518544269668]\n",
      "epoch: 600  loss: 0.32330076724463214  params: [-1.35003873  2.55297251 -2.430456  ]  gradients: [0.0005089899057922284, -0.0017700159014225913, 0.0016891317274523952]\n",
      "epoch: 700  loss: 0.3179661078934513  params: [-1.39744417  2.71654619 -2.58626283]  gradients: [0.00044217640248845355, -0.001514825362258923, 0.001440249499760975]\n",
      "epoch: 800  loss: 0.3140129978112108  params: [-1.43882485  2.85751807 -2.72007852]  gradients: [0.00038776048990994504, -0.0013141910845671017, 0.0012455082530623814]\n",
      "epoch: 900  loss: 0.31101101395141073  params: [-1.47525836  2.98049384 -2.83646395]  gradients: [0.00034272725818141453, -0.0011524491199722544, 0.0010891922304612067]\n",
      "epoch: 1000  loss: 0.3086862802954415  params: [-1.50757081  3.08881403 -2.93871283]  gradients: [0.00030496927654616936, -0.0010194169766460036, 0.000961122732243717]\n",
      "epoch: 1100  loss: 0.3068570739269473  params: [-1.53640811  3.18497987 -3.02928132]  gradients: [0.0002729470921948294, -0.0009081933230550561, 0.0008544253750897432]\n",
      "epoch: 1200  loss: 0.30539862197556245  params: [-1.56228313  3.27091401 -3.11005017]  gradients: [0.0002455127937485515, -0.0008139343017192093, 0.0007642915610766499]\n",
      "epoch: 1300  loss: 0.3042227861744153  params: [-1.58560883  3.34812702 -3.18249172]  gradients: [0.00022179724130084061, -0.0007331384970782969, 0.0006872567000042722]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.59173062,  3.36835455, -3.20144901])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd3 = gradient_descent(X_train, y_train, num_epoch = 10000, batch_size = 150)\n",
    "new_param_bgd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.28460936306821644  params: [-0.8575505   1.01823671 -1.14773408]  gradients: [0.025634681928999183, 0.013950418424516562, 0.01803632487396443]\n",
      "epoch: 100  loss: 0.07736674774641093  params: [-1.93032533  4.17501847 -4.0676902 ]  gradients: [0.0075385465472963716, 0.0041024842414178865, 0.005304051557227518]\n",
      "epoch: 200  loss: 0.07736266552438303  params: [-1.9303681   4.1751431  -4.06780374]  gradients: [0.007538159522074334, 0.004102273621921877, 0.0053037792498644536]\n",
      "epoch: 300  loss: 0.07736266518361752  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489767166, 0.004102273604340285, 0.005303779227133429]\n",
      "epoch: 400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 1000  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 1100  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 1200  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 1300  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 1400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 1500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 1600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 1700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 1800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 1900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 2000  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 2100  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 2200  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 2300  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 2400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 2500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 2600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 2700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 2800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 2900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 3000  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 3100  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 3200  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 3300  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 3400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 3500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 3600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 3700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 3800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 3900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 4000  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 4100  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 4200  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 4300  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 4400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 4500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 4600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 4700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 4800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 4900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5000  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 5100  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 5200  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 5300  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 5400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 5500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 5600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 5700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 5800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 5900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 6000  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 6100  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 6200  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 6300  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 6400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 6500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 6600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 6700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 6800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 6900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 7000  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 7100  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 7200  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 7300  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 7400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 7500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 7600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 7700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 7800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 7900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 8000  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 8100  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 8200  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 8300  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 8400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 8500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 8600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 8700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 8800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 8900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 9000  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 9100  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 9200  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 9300  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 9400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 9500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 9600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 9700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 9800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 9900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.9303681 ,  4.17514311, -4.06780375])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd3 = gradient_descent(X_train, y_train, num_epoch = 10000, batch_size = 1)\n",
    "new_param_sgd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.07217584526185061  params: [0.00935875 0.2214215  0.19528654]  gradients: [0.003514505088737236, 0.0032414982216910425, 0.004445097916030034]\n",
      "epoch: 100  loss: 0.02828443931190858  params: [-1.05266315  1.44008377 -1.35562159]  gradients: [0.0009664867320362995, 0.0011644430855933027, 0.001819856474024686]\n",
      "epoch: 200  loss: 0.023134744876513156  params: [-1.23284367  2.13126244 -2.02660745]  gradients: [0.000839905789491904, 0.0011066059384107262, 0.001580509605916502]\n",
      "epoch: 300  loss: 0.0205491356340153  params: [-1.35576914  2.56958074 -2.4468457 ]  gradients: [0.0007846220612652741, 0.0010790558881257444, 0.0014590708793663606]\n",
      "epoch: 400  loss: 0.018979473387216606  params: [-1.44666472  2.88048835 -2.74240646]  gradients: [0.0007544694076422675, 0.001062677544457727, 0.0013851563984215406]\n",
      "epoch: 500  loss: 0.01792659860986561  params: [-1.51650724  3.11481079 -2.96379177]  gradients: [0.000736388773478242, 0.001052323704836744, 0.001335845229830867]\n",
      "epoch: 600  loss: 0.017174166798861138  params: [-1.57173003  3.29814833 -3.1361992 ]  gradients: [0.000724770997443503, 0.0010454203326631425, 0.0013008598221287135]\n",
      "epoch: 700  loss: 0.016612499410014716  params: [-1.61632757  3.4452549  -3.27403243]  gradients: [0.000716901279863133, 0.0010406093882830484, 0.0012749294122596593]\n",
      "epoch: 800  loss: 0.016179793518422705  params: [-1.65292057  3.56543304 -3.3863084 ]  gradients: [0.0007113461605110325, 0.0010371342179159699, 0.0012550808149949652]\n",
      "epoch: 900  loss: 0.015838437256024674  params: [-1.68331286  3.66493556 -3.47905004]  gradients: [0.0007072938550316503, 0.001034549692560689, 0.0012395107090635423]\n",
      "epoch: 1000  loss: 0.015564159362589976  params: [-1.70879596  3.74817075 -3.55648019]  gradients: [0.0007042577661213657, 0.0010325810081185294, 0.0012270614604892606]\n",
      "epoch: 1100  loss: 0.015340558542829723  params: [-1.73032479  3.81836293 -3.62167264]  gradients: [0.000701932295293825, 0.0010310513448092465, 0.0012169554173877281]\n",
      "epoch: 1200  loss: 0.015156131848389796  params: [-1.74862411  3.87794011 -3.6769322 ]  gradients: [0.0007001179197615513, 0.001029842841119374, 0.0012086504799462639]\n",
      "epoch: 1300  loss: 0.015002561230236177  params: [-1.76425606  3.9287741  -3.72402906]  gradients: [0.0006986800315906637, 0.0010288745209568548, 0.0012017569698388594]\n",
      "epoch: 1400  loss: 0.014873676023903508  params: [-1.77766447  3.9723359  -3.76434981]  gradients: [0.0006975252340806809, 0.0010280892762740404, 0.0011959874508489677]\n",
      "epoch: 1500  loss: 0.014764798524827023  params: [-1.78920513  4.00980012 -3.79899842]  gradients: [0.0006965871310592287, 0.0010274459035568876, 0.0011911251995180027]\n",
      "epoch: 1600  loss: 0.014672316645084952  params: [-1.79916684  4.04211727 -3.82886597]  gradients: [0.0006958175067408389, 0.0010269140730628414, 0.0011870037033807335]\n",
      "epoch: 1700  loss: 0.01459339641356713  params: [-1.80778655  4.07006522 -3.85468005]  gradients: [0.000695180683698424, 0.001026471059688543, 0.0011834929289851164]\n",
      "epoch: 1800  loss: 0.014525783475920071  params: [-1.81526045  4.09428666 -3.87704062]  gradients: [0.0006946498170042007, 0.0010260995648519427, 0.001180489883214649]\n",
      "epoch: 1900  loss: 0.014467662882676686  params: [-1.82175229  4.11531693 -3.89644656]  gradients: [0.0006942044024580738, 0.0010257862326692499, 0.0011779119759370497]\n",
      "epoch: 2000  loss: 0.014417558026098985  params: [-1.82739961  4.13360509 -3.91331568]  gradients: [0.0006938285658394074, 0.0010255206188054414, 0.0011756922569490236]\n",
      "epoch: 2100  loss: 0.014374256465414178  params: [-1.83231865  4.14953007 -3.92800009]  gradients: [0.0006935098661850889, 0.0010252944609470227, 0.0011737759352762158]\n",
      "epoch: 2200  loss: 0.014336754592693436  params: [-1.83660811  4.16341331 -3.94079809]  gradients: [0.00069323844436709, 0.001025101154221843, 0.001172117793558554]\n",
      "epoch: 2300  loss: 0.014304215740259522  params: [-1.84035219  4.17552868 -3.9519636 ]  gradients: [0.0006930064079495444, 0.0010249353683649907, 0.0011706802385794176]\n",
      "epoch: 2400  loss: 0.014275938035655685  params: [-1.84362296  4.18611045 -3.9617136 ]  gradients: [0.000692807380456195, 0.0010247927645107991, 0.0011694318113725224]\n",
      "epoch: 2500  loss: 0.014251329431553304  params: [-1.84648233  4.19535968 -3.97023418]  gradients: [0.0006926361667995636, 0.0010246697830449364, 0.0011683460343518137]\n",
      "epoch: 2600  loss: 0.014229888089729728  params: [-1.84898361  4.20344945 -3.97768538]  gradients: [0.0006924885019384266, 0.001024563482830595, 0.001167400509010575]\n",
      "epoch: 2700  loss: 0.014211186811126347  params: [-1.85117285  4.2105291  -3.98420525]  gradients: [0.0006923608599404742, 0.0010244714180425573, 0.0011665762022920064]\n",
      "epoch: 2800  loss: 0.014194860559537642  params: [-1.85308989  4.21672782 -3.98991311]  gradients: [0.0006922503074115003, 0.0010243915428519658, 0.001165856876705821]\n",
      "epoch: 2900  loss: 0.014180596376592445  params: [-1.85476927  4.22215757 -3.99491234]  gradients: [0.0006921543898746478, 0.0010243221369597942, 0.0011652286311697366]\n",
      "epoch: 3000  loss: 0.014168125164023496  params: [-1.85624099  4.22691552 -3.9992926 ]  gradients: [0.0006920710428763576, 0.0010242617468961736, 0.0011646795280181742]\n",
      "epoch: 3100  loss: 0.014157214937970555  params: [-1.85753115  4.23108615 -4.00313184]  gradients: [0.0006919985218297791, 0.0010242091393558128, 0.0011641992877130308]\n",
      "epoch: 3200  loss: 0.014147665254105081  params: [-1.85866244  4.23474302 -4.00649789]  gradients: [0.0006919353461887238, 0.001024163263805485, 0.0011637790372290154]\n",
      "epoch: 3300  loss: 0.01413930257179613  params: [-1.85965468  4.23795022 -4.00944983]  gradients: [0.0006918802546780221, 0.0010241232222957177, 0.0011634111013531265]\n",
      "epoch: 3400  loss: 0.014131976377325717  params: [-1.86052514  4.24076365 -4.0120392 ]  gradients: [0.0006918321691255344, 0.0010240882449158496, 0.0011630888285678804]\n",
      "epoch: 3500  loss: 0.014125555925160174  params: [-1.8612889   4.24323213 -4.01431098]  gradients: [0.0006917901650396409, 0.001024057669704467, 0.0011628064450127107]\n",
      "epoch: 3600  loss: 0.014119927485915621  params: [-1.86195917  4.24539833 -4.01630447]  gradients: [0.0006917534475168922, 0.0010240309261034311, 0.0011625589314005636]\n",
      "epoch: 3700  loss: 0.014114992012372145  params: [-1.86254745  4.24729954 -4.01805403]  gradients: [0.0006917213313927114, 0.0010240075212509681, 0.0011623419188240363]\n",
      "epoch: 3800  loss: 0.014110663152438033  params: [-1.86306386  4.24896839 -4.01958971]  gradients: [0.000691693224793212, 0.001023987028564523, 0.0011621516001993565]\n",
      "epoch: 3900  loss: 0.014106865551637074  params: [-1.86351722  4.25043345 -4.02093782]  gradients: [0.0006916686154317634, 0.0010239690781826169, 0.0011619846547293508]\n",
      "epoch: 4000  loss: 0.01410353339842167  params: [-1.86391526  4.25171972 -4.02212139]  gradients: [0.0006916470591350946, 0.00102395334892565, 0.0011618381832617845]\n",
      "epoch: 4100  loss: 0.014100609174090999  params: [-1.86426476  4.25284913 -4.0231606 ]  gradients: [0.0006916281701913001, 0.001023939561504767, 0.001161709652809361]\n",
      "epoch: 4200  loss: 0.014098042575848006  params: [-1.86457167  4.25384088 -4.02407312]  gradients: [0.0006916116131956612, 0.0010239274727623323, 0.001161596848807895]\n",
      "epoch: 4300  loss: 0.01409578958693836  params: [-1.8648412   4.25471181 -4.02487446]  gradients: [0.0006915970961345568, 0.0010239168707694582, 0.001161497833936633]\n",
      "epoch: 4400  loss: 0.014093811672180165  params: [-1.86507791  4.25547668 -4.0255782 ]  gradients: [0.0006915843644982842, 0.0010239075706392408, 0.0011614109125241]\n",
      "epoch: 4500  loss: 0.01409207508073597  params: [-1.8652858   4.25614845 -4.02619627]  gradients: [0.000691573196253052, 0.0010238994109403063, 0.001161334599724032]\n",
      "epoch: 4600  loss: 0.01409055024087482  params: [-1.8654684   4.25673846 -4.02673912]  gradients: [0.0006915633975339573, 0.0010238922506163365, 0.001161267594777664]\n",
      "epoch: 4700  loss: 0.014089211233847022  params: [-1.86562878  4.2572567  -4.02721593]  gradients: [0.0006915547989453518, 0.0010238859663334633, 0.0011612087577860105]\n",
      "epoch: 4800  loss: 0.014088035335960127  params: [-1.86576966  4.25771191 -4.02763473]  gradients: [0.0006915472523752442, 0.0010238804501911884, 0.0011611570895048555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4900  loss: 0.014087002619573805  params: [-1.86589341  4.25811176 -4.02800261]  gradients: [0.0006915406282461089, 0.0010238756077429218, 0.0011611117137484213]\n",
      "epoch: 5000  loss: 0.01408609560509151  params: [-1.86600212  4.25846301 -4.02832577]  gradients: [0.000691534813137605, 0.0010238713562812217, 0.0011610718620490023]\n",
      "epoch: 5100  loss: 0.014085298957168202  params: [-1.86609761  4.25877156 -4.02860964]  gradients: [0.0006915297077272114, 0.001023867623349964, 0.0011610368602710617]\n",
      "epoch: 5200  loss: 0.014084599219313147  params: [-1.8661815   4.25904261 -4.02885901]  gradients: [0.0006915252250034283, 0.0010238643454515894, 0.001161006116921321]\n",
      "epoch: 5300  loss: 0.014083984581877521  params: [-1.86625519  4.25928072 -4.02907807]  gradients: [0.0006915212887131886, 0.0010238614669223487, 0.0011609791129325489]\n",
      "epoch: 5400  loss: 0.014083444679105741  params: [-1.86631993  4.2594899  -4.02927052]  gradients: [0.000691517832011043, 0.0010238589389525694, 0.0011609553927295804]\n",
      "epoch: 5500  loss: 0.01408297041151761  params: [-1.86637681  4.25967367 -4.02943959]  gradients: [0.0006915147962826862, 0.001023856718732567, 0.0011609345564124178]\n",
      "epoch: 5600  loss: 0.014082553790385113  params: [-1.86642677  4.25983511 -4.02958811]  gradients: [0.0006915121301191112, 0.0010238547687071713, 0.00116091625291314]\n",
      "epoch: 5700  loss: 0.014082187801501413  params: [-1.86647067  4.25997694 -4.0297186 ]  gradients: [0.0006915097884214294, 0.0010238530559247207, 0.001160900174002847]\n",
      "epoch: 5800  loss: 0.014081866285804703  params: [-1.86650923  4.26010154 -4.02983323]  gradients: [0.0006915077316190605, 0.0010238515514680987, 0.0011608860490409927]\n",
      "epoch: 5900  loss: 0.014081583834738374  params: [-1.86654312  4.26021101 -4.02993395]  gradients: [0.0006915059249863913, 0.0010238502299570844, 0.0011608736403735168]\n",
      "epoch: 6000  loss: 0.014081335698503204  params: [-1.86657288  4.26030719 -4.03002243]  gradients: [0.0006915043380452401, 0.0010238490691129953, 0.0011608627392985338]\n",
      "epoch: 6100  loss: 0.014081117705592847  params: [-1.86659903  4.26039168 -4.03010016]  gradients: [0.0006915029440419735, 0.001023848049377509, 0.0011608531625285452]\n",
      "epoch: 6200  loss: 0.014080926192209627  params: [-1.86662201  4.26046592 -4.03016846]  gradients: [0.0006915017194897455, 0.0010238471535788038, 0.0011608447490873723]\n",
      "epoch: 6300  loss: 0.01408075794033595  params: [-1.86664219  4.26053114 -4.03022846]  gradients: [0.0006915006437676153, 0.0010238463666390994, 0.0011608373575878804]\n",
      "epoch: 6400  loss: 0.01408061012339068  params: [-1.86665993  4.26058844 -4.03028117]  gradients: [0.000691499698769296, 0.0010238456753182943, 0.001160830863843293]\n",
      "epoch: 6500  loss: 0.0140804802585337  params: [-1.86667551  4.26063878 -4.03032749]  gradients: [0.0006914988685952932, 0.001023845067989206, 0.0011608251587708776]\n",
      "epoch: 6600  loss: 0.014080366164801612  params: [-1.8666892   4.26068301 -4.03036818]  gradients: [0.0006914981392830527, 0.0010238445344405384, 0.0011608201465520577]\n",
      "epoch: 6700  loss: 0.014080265926354982  params: [-1.86670122  4.26072187 -4.03040393]  gradients: [0.0006914974985702421, 0.0010238440657039478, 0.001160815743017211]\n",
      "epoch: 6800  loss: 0.014080177860212472  params: [-1.86671179  4.26075601 -4.03043534]  gradients: [0.0006914969356872098, 0.001023843653902455, 0.0011608118742277831]\n",
      "epoch: 6900  loss: 0.014080100487919224  params: [-1.86672107  4.26078601 -4.03046293]  gradients: [0.0006914964411748187, 0.0010238432921173112, 0.0011608084752312777]\n",
      "epoch: 7000  loss: 0.014080032510669508  params: [-1.86672923  4.26081236 -4.03048718]  gradients: [0.0006914960067246773, 0.0010238429742712573, 0.001160805488968136]\n",
      "epoch: 7100  loss: 0.014079972787460216  params: [-1.8667364   4.26083551 -4.03050848]  gradients: [0.0006914956250388821, 0.001023842695026001, 0.0011608028653117876]\n",
      "epoch: 7200  loss: 0.014079920315904684  params: [-1.86674269  4.26085586 -4.03052719]  gradients: [0.0006914952897069211, 0.0010238424496922332, 0.0011608005602256335]\n",
      "epoch: 7300  loss: 0.014079874215382794  params: [-1.86674823  4.26087373 -4.03054364]  gradients: [0.0006914949950976365, 0.0010238422341506547, 0.0011607985350227108]\n",
      "epoch: 7400  loss: 0.01407983371224084  params: [-1.86675309  4.26088943 -4.03055808]  gradients: [0.0006914947362643326, 0.0010238420447825746, 0.0011607967557154012]\n",
      "epoch: 7500  loss: 0.014079798126792938  params: [-1.86675736  4.26090323 -4.03057077]  gradients: [0.0006914945088615148, 0.0010238418784090533, 0.001160795192444351]\n",
      "epoch: 7600  loss: 0.014079766861902944  params: [-1.86676111  4.26091535 -4.03058193]  gradients: [0.0006914943090717521, 0.0010238417322374145, 0.0011607938189768102]\n",
      "epoch: 7700  loss: 0.014079739392954677  params: [-1.8667644   4.260926   -4.03059172]  gradients: [0.0006914941335414245, 0.0010238416038142397, 0.001160792612265956]\n",
      "epoch: 7800  loss: 0.014079715259042141  params: [-1.8667673   4.26093536 -4.03060033]  gradients: [0.0006914939793243662, 0.0010238414909841747, 0.0011607915520638783]\n",
      "epoch: 7900  loss: 0.014079694055230157  params: [-1.86676984  4.26094358 -4.03060789]  gradients: [0.000691493843832309, 0.001023841391853673, 0.0011607906205815435]\n",
      "epoch: 8000  loss: 0.014079675425755996  params: [-1.86677208  4.2609508  -4.03061454]  gradients: [0.0006914937247913953, 0.0010238413047591985, 0.001160789802190134]\n",
      "epoch: 8100  loss: 0.014079659058056422  params: [-1.86677404  4.26095715 -4.03062038]  gradients: [0.000691493620203931, 0.0010238412282392241, 0.0011607890831586113]\n",
      "epoch: 8200  loss: 0.014079644677521692  params: [-1.86677577  4.26096272 -4.03062551]  gradients: [0.0006914935283148782, 0.0010238411610097573, 0.0011607884514232914]\n",
      "epoch: 8300  loss: 0.01407963204288595  params: [-1.86677728  4.26096762 -4.03063001]  gradients: [0.0006914934475823397, 0.0010238411019427305, 0.001160787896385305]\n",
      "epoch: 8400  loss: 0.01407962094217787  params: [-1.86677862  4.26097192 -4.03063397]  gradients: [0.0006914933766516729, 0.0010238410500470654, 0.0011607874087327128]\n",
      "epoch: 8500  loss: 0.014079611189163297  params: [-1.86677979  4.26097571 -4.03063745]  gradients: [0.0006914933143327345, 0.0010238410044520285, 0.0011607869802842201]\n",
      "epoch: 8600  loss: 0.014079602620220046  params: [-1.86678082  4.26097903 -4.03064051]  gradients: [0.0006914932595799152, 0.0010238409643926302, 0.0011607866038518906]\n",
      "epoch: 8700  loss: 0.014079595091592016  params: [-1.86678172  4.26098195 -4.03064319]  gradients: [0.0006914932114745715, 0.0010238409291967688, 0.001160786273120513]\n",
      "epoch: 8800  loss: 0.01407958847697693  params: [-1.86678251  4.26098451 -4.03064555]  gradients: [0.0006914931692095937, 0.0010238408982739336, 0.0011607859825416148]\n",
      "epoch: 8900  loss: 0.014079582665407252  params: [-1.86678321  4.26098676 -4.03064763]  gradients: [0.0006914931320758966, 0.0010238408711053418, 0.0011607857272404034]\n",
      "epoch: 9000  loss: 0.014079577559388077  params: [-1.86678382  4.26098874 -4.03064945]  gradients: [0.0006914930994504845, 0.0010238408472351902, 0.0011607855029339292]\n",
      "epoch: 9100  loss: 0.014079573073261326  params: [-1.86678436  4.26099048 -4.03065105]  gradients: [0.000691493070786003, 0.0010238408262630153, 0.0011607853058592367]\n",
      "epoch: 9200  loss: 0.014079569131768535  params: [-1.86678483  4.26099201 -4.03065245]  gradients: [0.0006914930456015571, 0.0010238408078369757, 0.0011607851327102275]\n",
      "epoch: 9300  loss: 0.014079565668788282  params: [-1.86678525  4.26099335 -4.03065369]  gradients: [0.0006914930234746426, 0.0010238407916479551, 0.0011607849805821966]\n",
      "epoch: 9400  loss: 0.014079562626226798  params: [-1.86678561  4.26099453 -4.03065477]  gradients: [0.0006914930040340482, 0.0010238407774243589, 0.001160784846923088]\n",
      "epoch: 9500  loss: 0.014079559953043228  params: [-1.86678594  4.26099557 -4.03065573]  gradients: [0.0006914929869536339, 0.001023840764927569, 0.0011607847294906827]\n",
      "epoch: 9600  loss: 0.014079557604393509  params: [-1.86678622  4.26099648 -4.03065656]  gradients: [0.0006914929719468606, 0.0010238407539479447, 0.0011607846263149892]\n",
      "epoch: 9700  loss: 0.014079555540877873  params: [-1.86678646  4.26099728 -4.0306573 ]  gradients: [0.0006914929587619784, 0.0010238407443012975, 0.0011607845356651858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9800  loss: 0.014079553727879831  params: [-1.86678668  4.26099798 -4.03065795]  gradients: [0.0006914929471777915, 0.001023840735825786, 0.0011607844560205661]\n",
      "epoch: 9900  loss: 0.014079552134985608  params: [-1.86678687  4.2609986  -4.03065852]  gradients: [0.0006914929369999729, 0.0010238407283792357, 0.0011607843860450657]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.86678704,  4.26099914, -4.03065901])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd3 = gradient_descent(X_train, y_train, num_epoch = 10000, batch_size = 72)\n",
    "new_param_mgd3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 4. learning_rate = 0.1, num_epoch = 1000, tolerance = 0.001\n",
    "tolerance 감소  \n",
    "- 그레디언트 벡터의 norm이 허용오차(tolerance)보다 작아지면 GD가 거의 최솟값에 도달했으므로, tolerance가 감소하면 그만큼 반복횟수는 증가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.9525312383397856  params: [0.00696778 0.95432555 0.85044846]  gradients: [0.021679977783459897, 0.016815240768904588, 0.03672601800619051]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.84447549,  0.84957239, -0.71683273])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd4 = gradient_descent(X_train, y_train, tolerance = 0.001, batch_size = 150)\n",
    "new_param_bgd4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.2847814878818582  params: [-0.8511337   1.03042199 -1.1652916 ]  gradients: [0.02564839655203979, 0.013957881935492859, 0.018045974355762936]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.1107857 ,  1.66594158, -1.6882428 ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd4 = gradient_descent(X_train, y_train, tolerance = 0.001, batch_size = 1)\n",
    "new_param_sgd4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.12901964116875772  params: [0.62999923 0.77197119 0.31218432]  gradients: [0.005345689295156522, 0.0048408429869881845, 0.006043578200364318]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.44901537,  0.73540604, -0.53312455])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd4 = gradient_descent(X_train, y_train, tolerance = 0.001, batch_size = 72)\n",
    "new_param_mgd4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_mgd3)\n",
    "    if p> 0.5 :\n",
    "        y_predict.append(1)\n",
    "    else :\n",
    "        y_predict.append(0)\n",
    "y_predict_random = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
    "    if p> 0.5 :\n",
    "        y_predict_random.append(1)\n",
    "    else :\n",
    "        y_predict_random.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38,  2],\n",
       "       [ 1,  9]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn) / (tp+fn+fp+tn)\n",
    "print(\"accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "### $y = 0.5 + 2.7x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = np.random.rand(150)\n",
    "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([1 for _ in range(150)])\n",
    "X = np.vstack((tmp, raw_X)).T\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45289187, 2.60249534])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규방정식\n",
    "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.652908043149421  params: [0.20763035 0.45493109]  gradients: [1.717237094248022e-05, 8.521360265637006e-06]\n",
      "epoch: 100  loss: 1.6625043377877846  params: [0.20330336 0.45240731]  gradients: [1.7226891388218074e-05, 8.547648731687982e-06]\n",
      "epoch: 200  loss: 1.6721670973073632  params: [0.19895971 0.44987486]  gradients: [1.7281617163732028e-05, 8.574035546491233e-06]\n",
      "epoch: 300  loss: 1.68189680053088  params: [0.19459932 0.44733371]  gradients: [1.7336549040268183e-05, 8.600521079329329e-06]\n",
      "epoch: 400  loss: 1.6916939298049936  params: [0.19022214 0.44478383]  gradients: [1.7391687791969147e-05, 8.627105700871696e-06]\n",
      "epoch: 500  loss: 1.7015589710265198  params: [0.18582811 0.44222517]  gradients: [1.7447034195884904e-05, 8.653789783179826e-06]\n",
      "epoch: 600  loss: 1.7114924136688519  params: [0.18141715 0.43965772]  gradients: [1.7502589031983745e-05, 8.680573699712504e-06]\n",
      "epoch: 700  loss: 1.7214947508085838  params: [0.17698921 0.43708144]  gradients: [1.7558353083163215e-05, 8.707457825331069e-06]\n",
      "epoch: 800  loss: 1.7315664791523142  params: [0.17254422 0.43449629]  gradients: [1.761432713526112e-05, 8.734442536304644e-06]\n",
      "epoch: 900  loss: 1.741708099063678  params: [0.16808212 0.43190224]  gradients: [1.7670511977066585e-05, 8.761528210315471e-06]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.16364771, 0.42932533])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#경사하강법\n",
    "new_param = gradient_descent(X, y, model = 'linear', learning_rate = 0.00001, batch_size = 72)\n",
    "new_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_NE = theta.dot(X.T)\n",
    "y_hat_GD = new_param.dot(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
    "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3df5xVdb3v8dd39jAzXIXAgfwBIiD9kNQMp3IobYQkM/Nnprd8UOgJMPVxATscoaNh+IBUUjI9Blcx8cchjz+SU/dGJ2SSnE01BNf8kQYonclEwKAShoGZ7/1jzcAws/aetfdev/f7+Xjsx2Nm1tprfdfstT77+/18v9+1jLUWERFJroqoCyAiIqVRIBcRSTgFchGRhFMgFxFJOAVyEZGEq4xip0OGDLEjR46MYtciIom1fv36HdbaoT3/HkkgHzlyJM3NzVHsWkQksYwxW93+rtSKiEjCKZCLiCScArmISMJFkiN3s3//flpaWmhtbY26KCWpqalh+PDh9OvXL+qiiEiZiE0gb2lpYcCAAYwcORJjTNTFKYq1lp07d9LS0sKoUaOiLo6IlInYpFZaW1upra1NbBAHMMZQW1ub+FaFiCRLbAI5kOgg3iUNxyCSNNlsloULF5LNZqMuSiRik1oRESlGNptl4sSJtLW1UVVVxerVq6mvr4+6WKGKVY08asYYbrjhhoO/L1q0iHnz5gEwb948hg0bxmmnnXbwtWvXrohKKiJdGhsbaWtro729nba2NhobG6MuUugUyLuprq7mqaeeYseOHa7LZ86cycaNGw++Bg0aFHIJRaSnhoYGqqqqyGQyVFVV0dDQEHWRQqdA3k1lZSVTp07lrrvuirooIuJRfX09q1evZv78+WWZVoGY5shnzICNG/3d5mmnweLFfa937bXXcuqppzJ79uxey+666y4eeeQRAAYPHsyaNWv8LaSkTjabpbGxkYaGhrIMMGGpr68v6/9vLAN5lAYOHMjkyZO5++676d+//2HLZs6cyTe+8Y2ISiZJo044CUssA7mXmnOQZsyYwbhx45gyZUq0BZFEc+uEUyCXIChH7uKoo47ii1/8Ig888EDURZEEUyechEWBPIcbbrih1+iVu+6667Dhh2+88UY0hZNEUCechMVYa0PfaV1dne35YIlXXnmFk046KfSyBCFNxyJSCHXuBssYs95aW9fz77HMkYtI8qhzNzpKrYiILzTDMjoK5CLiC3XuRkepFRHxRVfnrnLk4VMgFxHflPsMy6gotSIiUqS43AddNfJutm3bxsyZM1m3bh2DBw+mqqqK2bNnM3jwYC688EJGjx7Nnj17OProo5k9ezbnn39+1EUWkYjEaZSOauSdrLVcdNFFnHXWWWzZsoX169ezYsUKWlpaADjzzDPZsGEDr776KnfffTfXXXcdq1evjrjUIpJPkDXmOI3SUSDv9Oyzz1JVVcX06dMP/u2EE07g+uuv77Xuaaedxs0338w999wTZhFFpABdNeabbrqJiRMn+h7M4zRKJ56plQjuY/vSSy8xbtw4z5sbN24cd9xxhx8lE5EABH3TsjiN0vGtRm6MyRhjNhhjfuLXNqN07bXX8uEPf5iPfvSjrsujuLWBiHgXpxpz0Pyskf8v4BVgYMlbiuA+th/60Id48sknD/5+7733smPHDurqet3WAIANGzbofioh0j08pFBB15hT19lpjBkOfA6434/tRWHChAm0trZy3333Hfzbnj17XNd94YUXmD9/Ptdee21YxStrQec6Jb3q6+uZM2dOIAE2Tp2dftXIFwOzgQG5VjDGTAWmAowYMcKn3frHGMOPf/xjZs6cye23387QoUM54ogjuO222wBYu3YtH/nIR9izZw/vfe97ufvuu5k4cWLEpS4PekCDxFFX6qarRp7ozk5jzPnA29ba9caYhlzrWWuXAkvBuY1tqfsNwrHHHsuKFStcl+3evTvk0kiXOF0wIl3i1NnpR438E8AFxpjzgBpgoDHmEWvtlT5sWyRWF4xId3G5JUHJgdxaOweYA9BZI/+Ggrj4LS4XjIgXYXfOx2ocubUWY0zUxSiJhiWKRCMuI5uiGM3iayC31jYCjcW8t6amhp07d1JbW5vYYG6tZefOndTU1ERdFIm5uASdtIjTUMAoOudjUyMfPnw4LS0tbN++PeqilKSmpobhw4dHXQyJsTgFnbSI08imKDrnYxPI+/Xrx6hRo6Iuhkjg4hR0kiZXS6bY4BlEyyiKzvnYBHKRcqHhlMXJ15IpJngG2TIKu3NegVwkZBpOWZy+WjKFBs80tYwUyEUioOGUhfO7JeN1e0nomFYgF5FE8Lsl42V7SemYViAXSbhSaoxJqG1253dLpq/tJSX9okAukmCl1BiTUtuMUlI6pvWoN5EEK+VWqnG6DWtcdaVf5s+fH+svOtXIRRKslBpjUmqbUUtCx7QCuUiCldIBqGGQ6WGiuMlTXV2dbW5uDn2/IiJJZoxZb63t9fxJ5chFRBJOgVwkItlsloULF+oZpFIy5chFIqChf+In1chFIqChf+InBXKRCHQN/ctkMhr6l2I902dBpdOUWhGJgIb+pV/P9NnixYuZMWNGIOk0BXIpW1HfZyQJE02keD3TZ08++WRg921RIJeypM5GCVrPmbOXXnopa9euDWQmrQK5lKWk3NVO/BdWS8wtfXbKKacEsm8FcilLus9IeQq7JdYzfRZUOk2BXMpS99pSbW3tweF/qpWnW1pbYiUHcmNMDfAcUN25vSestd8qdbsiQeu6gJUrLx9pbYn5MY58HzDBWvth4DTgXGPMGT5sVyRwmphTXqK4v/hrr8HFF4Mxzmv9ev/3UXKN3Dq3T/xH56/9Ol/h31JRpAhpraGljZ8dlEEP+9y7FxYvhrlz3Zcfd5z/+/QlR26MyQDrgTHAvdbaX7usMxWYCjBixAg/ditSMk3Mib8kDBVdswZmzYKNG3svq6yEO++EadOgqiqY/fsyRd9a226tPQ0YDnzMGHOyyzpLrbV11tq6oUOH+rFbkYLpjoPJ4yX9Ffbnum0bXHPNoXTJhAmHB/EvfQlefx2shf374frrgwvi4POoFWvtLmNMI3Au8KKf2xYplVvNDtTZGXd9pb/CqLG3t8PDDzu17r/+tffyMWOcWvf55zuBPWwl18iNMUONMYM6f+4PfBr4Q6nbFfGbW81OnZ3x11cHZVCf4Ysvwmc/6wTmykqYMuXwID53Luze7dS6//hH+Pznowni4E+N/Fjgoc48eQXwuLX2Jz5sV8RXuWp26uwMlh8dlfk6KP3qsH73XVi0CObNc18+YQLccQeMG1fU5gPlx6iVF4CP+FAWkUDl6thUZ2dwgkp79PxyKOYztBbuvdfJX7v7O/36zWH16i9z5pnxPi80s1PKilvNTnchDE4QMylzfTl42e4rr8D48bBrl/vyq66CoUO/z6JFM2lvb6ejI8OvfjUs9oFcD5YQkcAE8QANrznxbDbL/Pm3ccEF2w6OLhk7tncQv+cep3ZuLTzwAFx4YV3iHvqhGrmkQtT3Fhd3fo7T7/qMa2tr8+bEf/pTZ/QI1He+DjdhAjzxBAweHHyZw2KciZnhqqurs83NzaHvV9IpCRNGpDRuT9vZuXMnDQ0NnHhiPZ/7HOQLKVdf/Sj33//l8AocEGPMemttXc+/q0YuiZfWO9rJId0/43372njqqfexatXUnOtffvmbPPPMB9i/fy9VVVVcffXqEEsbPgVySTzdLyX9jjvuPNrbZwHVdHTAqlWHLx8xwpkmP3r0wXeQzf7cc3ok6NRc0NtXakVSIawceTabZfny5QBMnjxZNf+A7N3rjCBZsSL3Og8+CF/9aun7Cjo15+f2lVqRw6StczCMIYTZbJazzz6bffv2AbBs2bKDaZy0/T+j8MQTcNlluZeffz489hgMGODvfoNOzYWR+lMgL0PqHCxO1wXZZf/+/QeHvun/6SjkC+3Pf4ZJk+Dll3Ov8/zzzrjvIAWdmgsj9adAXobUOVicrguyq0ber18/Ghoa9P/s1FcFoaMDvvUtuPXW3Nv413+FW26BipBmuHR98XQfBeP3ZxfGcEYF8jJUSg2h2BRCGlIP9fX1rFmzxjVHrs5W9wpCRUU9Z+R5XtgHPwj/9V8wfHhp+y7m/AqzZRp46s9aG/rr9NNPtxKtpqYmu2DBAtvU1FTQe/r3728zmYzt37+/5/cW8r5iyhUHS5YssZMmTbJLliyJuiiRaWpqsjU1Qyw8Yw/Nlez9WrHC//0Wc14uWLDAZjIZC9hMJmMXLFjgb8ECADRbl5iqGnmZKqaGUGwKwev7kpq7z2azzJgxg7a2NtauXcspp5ySiHL7Zfly+MpXwJlFub3X8iuugGXLoH//3u/1o6VW7HmZpmGrCuTiWbEnvtf3JTXXnNRyF+uNN6ChAbZudV9eWQm//nXft3v164u72PMyiVPxc0llIE9DPjaOij3xvb4vity9H9JUs3PT3g6zZztPwMllwQK48cbCHqzg1xdgKQE5NXe+dMu3BP0KMkdebL5M4iHM3L2fkprbz6WxMXeOG6w9/XRr33qrtH3E4XPzIk6fLeWSIy+3Zm7ahJm7L1XPVkCx+4xDC3LXLvjCF2B1nluSrFzpPM7ML0lIbSSl3yZ1gTztzVzpLYrPPNcFXmhQjjJQ/OAHzpPgc5kyBe67D6qrgytD3FMbSakYpi6QF/otH4fakJQmipqd2wUOhc/wDDNQvPoqfPKTsGOH+/L3vMeZSfmhDwWy+0RKTMXQLd8S9Csu48ijytHFKecmxXE7d4oZlxzkOdjWZu20aflz3d/7nrUdHb7tMpXidL1SLjnyQkTRbCq0Ka0WQzzlagUUWnsrtTXR8/xYtQrOPTf3+meeCU8/DbW1Be2mrPVM/8TymnSL7kG/yrlGXkitbcmSJbZfv362oqIi1r36ckiYtTdnJuVwC015a90//3ngRYlEFDXlqEfakKNGXtaB3NrwTwavJ0JTU5OtrKy0gAVsRUVFIqYQS7A6Oqy944786ZLrrrN2//6oS9pboddavvWjCqhRT+vPFcjLOrUC4feae21KNzY20tHRcfD3TCYT344WCdTvfw9nnAF79uRa402qqyexZs3/jm3nfjEpxXzrRzWaJK6dnyUHcmPM8cBy4BigA1hqrf1eqdtNMy9fHg0NDVRXV7Nv3z4qKiq455574pOPk0C1tsLUqfDww7nXufHGzQwc+Di1tbWdt1/1FsSjGupYaODta/2oAmoxfRqhfHm6VdMLeQHHAuM6fx4AvAaMzfeeOKVWgtCzSVhs+iZOveUSrKefzp8u+exnrd2921k3iXf7K7TMXtZPwvXhdwqIsHLkwDPAOfnWSXMg7/nBLVmyJO8HmYSTUfz35pvWnnpq/uC9dq37e4sNyFF31PWV8+65LA3Xht9fnqEEcmAk8CdgoMuyqUAz0DxixIiSDibOen5wkyZNyvlBRn1hSXg6Oqy95Zb8gfvGG609cKDvbZVy3sQxOKb5fvWJq5EDRwLrgUv6Wlc1ckfUPeASrN/+Nn/gHjPG2q1bi9t2VEPvgtin1+sgqRUfP/9vgQZyoB+wCpjlZf04BPIgLwSvOfKknpji7h//sPaSS/IH70ceibqUxQnyXPW67ThUfKJuEQQWyAGDM2plsdf3RB3I4xRAoz4xyl2p//9HH80fuC+91AnwSRd0EG1qarLTp0+306dPzzu3Iuocf9RxI8hA/kmcSSsvABs7X+fle0/UgTwO3+wSrlydaYVemFu3WnviifmD929+E+SRRCPoIFbIRLmoKj5xiBu5AnnJ48ittb/qrJUnRlwH9Uswco2f9jK2uaMD5syB22/Pvf1vfxsmTszyy186Y4U/+lHvY4Vjed8OF0HfYdLrOPMob3sb67jhFt2DfkVdI7dWKY1ykqsmlasWuHZt/hr3qac6wwe7FFtbLWZsdVrP2TikLbyI+jNAU/QPF/cb2ot/ctWkumqZP/tZE6tWXc348YNybuOpp+Dii92X5apN9lXbLmS2Y1KeVFOsJDwtCOIbN8o2kEv5cAsS998PX/saQH3n63DGPIy1XyOTOcD8+fO5+OI5Obfv9kXhJfAW0lRPypNqShHXIJkEFVEXQOIrm82ycOFCstls1EVxVUj5hg6t5847b2D8+HqM6QrihxxxBLzwgpM8aWrKUlMzjUzmgKdcaNcXxfz58/Pm3728L5euoJ/JZOKXn5XoueVbgn7FIUdeiKjzYlGIe86yr/Lt3+/czjVfrvu73839dJxSP/Mg/n/leB7K4VCOvDhpz03mEvemvFv53n23nnPOyfeuJuACMpldzJ8/n1mzcqdLSm3m58v5FjtSRakHySU1gTyoYVxxD2hBifVQK6C2thZjaoH/oL39LObOdV/vq1/9EQ8//GXa29sBMMZQVVUTyvG4Bd5yrRhIsFKRI++6OG666SYmTpzoa063XHOTheRvw2ItLF4MxsC0aVM5cGAbcNZh61xzDbS1HUqgTJ064rDPb9q0aZEej5fcuUihUlEjD7LWHKdhUWFPHolDU/6ll2D8ePjb33Kt8TYzZz7NnXdOc10ap88P4t/SkYRyS5wH/fK7szPuHXN+SPIxFtJJt2+ftVOm5O+krKy8zlZVVSXyf2GtOi2leKS5szNuta4gJDVX7yUn/J//CRdckHsb55wDZ5xxJwsWzKa9vR1rM1x11dcYMWJEIj/vOLR0JF1SEcgh/RdHnJrkhaR43L6ARo+u57zz4He/y/2+NWug+yFms/UsWnTo+CdPnlzy552U+5yI9Mmtmh70KynjyOPWBI5DeYq5P0hNTX9rzDfzpktmzer76TjFHr9fdz4stCxx+LwkXQjrmZ1eXkkI5EnOSQfJ6608f/c7azOZ3IF75Ehrt2wJvry5PsdSbknq5dyI+/mjL5lkyhXIUzH8MAhpGybm13T7XMMx9+6Fyy93hgYaA+PGQefQ7YMeeuhQKH/9dRg1qqSieJLrcyxlWKmXcyPO50+Qw3UlGqnJkfstTjnpUvk5CaV7x3J7+6WMH//+nOtecAE8+igceWSxJS9dX3c+LCZH7uXc8Hr+RJGnT2rHueSmQJ5DoRd6nDvOctUOCy1vS4szguQPf3C/YyDAunXw8Y/7VHAf5Psci+0g93JueFknqlmeaaqkhC2217lbviXoVxJy5IVIQj60e/mWLFly8Pfq6uqcz0lsb7d27tz8Y7pvvtlZLwp+53nDzhtH+egw5cgLF4frHHV2BicOz/LrS/cLt3t5AWuMOXhiLlnyQt7APXastS0tUR9NYReV1xEmYV+kcQgM4l0crvNcgVypFR8koanaM41QVVVFa2sr1h6BtY+xd+/nGT/e/b2PPw6XXRZSQT3ymuf1mr6IIm8cxB0SJThxvs4VyH2QtJmlra317N27J88aj1JRMY1bb/0mc+bkvtVrlLxeVF4DdFQXaZLvkFhOXzZdx7p48WJ27twZv2N2q6YH/UpbaiXu3nrL2unTc6dLMpn99oc/3Ji4pr7fKZO45I3j0ITvS9LOlVLE6VhRaqV8tLfDI4/ArFnwzju9l48eDfPmwZVXOmO+nYbZhwES1bLwMuqkkNZSXG7zEOcmfJdyGsKYhGP1JZAbY5YB5wNvW2tP9mObUpgXX4R//mf42c/cl8+dC//yLzBwYP7txCWY+SmOx5QvLZGEVF0Svmz8koRjNU5tvcSNGHMW8A9guZdAXldXZ5ubm0vebzl7911YtMipWbuZMAHuuMOZYSnxkpQceF/KMUce9bEaY9Zba+t6/t2XGrm19jljzEg/tiW5rVrlpEtefrn3siOOgO9+F66+GiqVMIu1MJrqYQSeOLZ0ghL3Y9UlH2N//jPcdBM8+KD78quugm9/G4YNK277Ydcy4lKriVrQTfW01PjFu9ACuTFmKjAVYMSIEWHtNlEOHIAHHnBq3XtcRgeOHQt33gmf+Uzp+wr7YldwOSToHHgSOufEX6Hd/dBau9RaW2etrRs6dGhYu429DRtg4kRn9Ei/fjB9+uFBfN48+PvfnYGCL73kTxCH8O/OF/b+4q6+vp45c+YEEmBLubOjJJNSKyH729/gO9+BhQvdl593Htx2G5wc8NifsHvik9DznxZJGPUi/vJr1Mq/Aw3AEGAb8C1r7QO51i+nUSvWwsqVTrpky5bey486ykmXXHklZDLhlk05cpFkyTVqxZdAXqi0B/KtW51x24895r58+nQnZXL00aEWK3AK1CLBCnT4Yblra4P77oMbbuj9VByAD3zgH9x335GcfXb4ZQuLOjNFoqNHvRVp3Tr4xCecTsrqapgx4/Agfs01W6mqGgQYNm0axB//uDSysoZBnZki0VEg9+idd5wad9czKevroanp0PKLLoI//KH7rai+Q1vbbgDa29v5+te/nupnI2qkhEh0UpFaCSI3ay088YTTSdnS0nv5scc6MykvvxwqPHwddnR0pHo8r0ZKiETI7ZaIQb/8vI2tn7eYfO01ay+5JPftXmfMsHbHDu/lqqysPPgUnurq6lTf6lNEgkdab2Nbyiy21lb4/vdh9mz35Wec4dS6cz05J5/6+nqee+45li9fDsDkyZMT/QBnEYmvRAVyt0BX6EST555zct1uox8rKpzAfc01TgdmqQq50Y5GfURDX56SBokJ5LkCXV+52bffhltugX/7N/ftXnEFLFgAo0aFcBB56P4Y4dOXp3i2Zw+89RZs2+a88v387rv5t/WXv8Axx/havMQE8nyBrnvNt6MDHn3U6aTcsaP3dkaOdGZSXnRR19Nx4kFT2MOnL88Uevdd2LzZeW3a5Eyn7vr59dejLp1zQ6WaGt83m5hAni/QvfKKk+f+yU/c3zt7NsyZA4MGhVPWYhQz6kNpgdLoyzNCra2Hgqxb4HWbWRel6mqnFn300c7rmGN6/97184ABodcSEzVFvytwffzjE3j++Y9z883u6zU0OE/Hqes1kTU9lBbwh74M+9DWBm+8cXjA7R5429qiLmF+Q4Y4D6kdMwZOPPHQa8wYJ+jGqVnuQSqm6L/5Zj1z5/a+2GpqnHTJP/2T03IpB0oLFM4taMf9yS+eHTgAf/qTe8DdvLnvvG3U3vMe92A7ejQcd1z4d5RLmEQF8oceOvTz5Mlw661w/PHB7S/OtTWlBQoTmxZMR4czw2zzZjb//Odsy2Z5fybDkF27nIC7e3f4ZSrEkUe6B9sTT3QuRj1nMBKJ+q+vXOnPdrwE6GIufK/b9ePLQTMpC1NwC8ZaZwTCpk3uNdydO0su04mdr0DU1LgH2zFjYMQIqKoKas8SgUQFcj94DdCFXvhetut3rTA1aYFSWAvbtx8+OqF7wN22DYA5nS/A6UibO9d5RewAsAmo/MAHGDNp0uGBd+RI6N8/4hJKEpRdIPcaoAtNXXjZbhB57TDSP77t469/dR8Wtnmz86TpuOtZs+36efRoOOKIw1bt63/W60v9wQcZU+5fylK0sgvkXgN0oakLL9v1O68dWN73738/GGy3PvssryxZwsfa2znWGKcGHHcnnOCeUhg9GgYODHz3Xj4XpcbET2UXyAu5gApJXXjZrt8X79pf/IL37tvHkI4Ohu3bxzuLFsHpp/eeafbWW0V3op0AXNX1i99BfNiw3LXco47yd18h8tryUmpM/JKoceSJdeCAc68AL1N8fehE89tbwBbg9UyGs6ZM4fhPfepQ4B0yJHFjcYMWmxEykjqpGEceuPZ2Z15/95psrsC7fXvUpYXKSvYNHsw7VVXUnHACgz/4wd6zzLp+HjSo6ID7ejbLLztbEccrIPVJaRMJWzJr5B0dziN73FIIboE36ryuMfmn9Hb/+aijDj6pQjU7EekuHTXyxx93HskTpK6Ami/YHnMM1NYGPttMszeTI86TxyT9khXITz7ZCZ7t7U5utq9ge8wxznoJnW2m2ZvJoJaTRC1ZEW7sWKfjsEwo15oMajlJ1HwJ5MaYc4HvARngfmvtd/zYbiHS2rTVELXihXVOqOUkUSs5kBtjMsC9wDlAC/BbY8xKa+3LpW7bqzCbtqUGh2w2W9BzPJMgjl+iuc6JIMqqlpNEzY8a+ceATdbaLQDGmBXAhUBogTyspm2pXxjZbJazzz6bffv2AbBs2bLEN8Pjmh92OyeAwMqqlpNEqcKHbQwD/rvb7y2dfzuMMWaqMabZGNO83ecx2F1N20wmE2jTdvny5bS2tvYKDl51BZcu+/fvL3gbcZMrYEbN7ZyIa1lFSuVHjdxtlkmvgdvW2qXAUnDGkfuw34O8NG39SIksW7aMrnH3lZWVBX9hdAWXrhp5v379Ep9PjWt+ONc5EceyipTKj0DeAnR/vMNw4E0ftluQfE1bP5r/jY2NtHc+R9AYw5QpUwreRn19PWvWrElVjjzO+eGe50ScyypSCj8C+W+B9xljRgF/Bq4AvuTDdn3jRw69Z81z8uTJRZUljbnUJB1Tksoq4lXJgdxae8AYcx2wCmf44TJr7Usll8xHfjT/41qbi+OIEREJVzLvtVKENAa8uI4YEZFgpONeKyVIY5NaMwpFBPwZfigRCWvYpYjEW9nUyNMornl7EQmXAnnCpTFlJCKFUWpFRCThFMhFRBJOgVxEJOEUyMtMNptl4cKFZLPZqIsiIj5RZ2cZ0QQikXRSjbyM6DauIumkQF5GNIFIJJ3KIrWSxvusFEMTiETSKfWBXHnhw2kCkUj6pD61orywiKRd6gO58sIiknapT60oLywiaZf6QA7KC4tIuqU+tSK5aZanSDqURY1cetNoHpH0UI28TJXDaB61OKRcqEZeprpG83TVyNM2mkctDiknCuRlKu2jefRgaiknCuRlLM2jedLe4hDpToFcUintLQ6R7koK5MaYy4B5wEnAx6y1zX4USsQPaW5xiHRX6qiVF4FLgOd8KIuIiBShpBq5tfYVAGOMP6UREZGChTaO3Bgz1RjTbIxp3r59e1i7FRFJvT5r5MaYXwDHuCz6prX2Ga87stYuBZYC1NXVWc8lFBGRvPoM5NbaT4dREBERKY6m6MeIppSLSDFKHX54MfB9YCjwU2PMRmvtZ3wpWZnRlHIRKVZJNXJr7dPW2uHW2mpr7dEK4sUrh5tYif/HmNgAAASGSURBVEgwlFqJCT2STkSKpSn6MaEp5SJSLAXyGNGUchEphlIrIiIJp0AuIpJwZRPINUZbRNKqLHLkGqMtImlWFjVyjdEWkTQri0CuMdoikmZlkVrRGG0RSbOyCOSgMdoikl5lkVoREUkzBXIRkYRTIBcRSTgFchGRhFMgFxFJOAVyEZGEM9aG/0B7Y8x2YGsRbx0C7PC5OHFXjscM5XncOubyUMoxn2CtHdrzj5EE8mIZY5qttXVRlyNM5XjMUJ7HrWMuD0Ecs1IrIiIJp0AuIpJwSQvkS6MuQATK8ZihPI9bx1wefD/mROXIRUSkt6TVyEVEpAcFchGRhItlIDfGnGuMedUYs8kYc6PL8mpjzI86l//aGDMy/FL6y8MxzzLGvGyMecEYs9oYc0IU5fRTX8fcbb0vGGOsMSYVw9S8HLcx5oudn/dLxpjHwi6j3zyc3yOMMWuMMRs6z/Hzoiinn4wxy4wxbxtjXsyx3Bhj7u78n7xgjBlX9M6stbF6ARlgMzAaqAL+HzC2xzpfB37Q+fMVwI+iLncIx3w28D86f76mHI65c70BwHPAOqAu6nKH9Fm/D9gADO78/b1RlzuEY14KXNP581jgjajL7cNxnwWMA17Msfw84P8CBjgD+HWx+4pjjfxjwCZr7RZrbRuwAriwxzoXAg91/vwEMNEYY0Iso9/6PGZr7Rpr7Z7OX9cBw0Muo9+8fM4A84HbgdYwCxcgL8f9NeBea+1fAay1b4dcRr95OWYLDOz8+T3AmyGWLxDW2ueAd/KsciGw3DrWAYOMMccWs684BvJhwH93+72l82+u61hrDwC7gdpQShcML8fc3dU43+RJ1ucxG2M+Ahxvrf1JmAULmJfP+v3A+40xzxtj1hljzg2tdMHwcszzgCuNMS3A/wGuD6dokSr0us8pjo96c6tZ9xwj6WWdJPF8PMaYK4E64FOBlih4eY/ZGFMB3AV8NawChcTLZ12Jk15pwGl5rTXGnGyt3RVw2YLi5Zj/J/BDa+13jTH1wMOdx9wRfPEi41sci2ONvAU4vtvvw+ndzDq4jjGmEqcplq8JE3dejhljzKeBbwIXWGv3hVS2oPR1zAOAk4FGY8wbODnElSno8PR6fj9jrd1vrX0deBUnsCeVl2O+GngcwFqbBWpwbi6VZp6uey/iGMh/C7zPGDPKGFOF05m5ssc6K4GvdP78BeBZ29l7kFB9HnNnmmEJThBPes4U+jhma+1ua+0Qa+1Ia+1InH6BC6y1zdEU1zdezu8f43RuY4wZgpNq2RJqKf3l5Zj/BEwEMMachBPIt4dayvCtBCZ3jl45A9htrf1LUVuKumc3T2/uazg93d/s/Nu3cS5kcD7k/wA2Ab8BRkdd5hCO+RfANmBj52tl1GUO+ph7rNtICkatePysDXAn8DLwe+CKqMscwjGPBZ7HGdGyEZgUdZl9OOZ/B/4C7MepfV8NTAemd/uc7+38n/y+lPNbU/RFRBIujqkVEREpgAK5iEjCKZCLiCScArmISMIpkIuIJJwCuYhIwimQi4gk3P8HYuypJRWoqB0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
    "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
    "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
