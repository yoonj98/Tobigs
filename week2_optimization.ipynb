{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tobigs 2주차 Optimization 과제\n",
    "#### 15기 이윤정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Gradient Descent 구현하기\n",
    " 1)\"...\"표시되어 있는 빈 칸을 채워주세요  \n",
    " 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.25 : 0.75로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때, scaler는 X_train에 fit되었으므로 X_test를 scaling할 때에는 transform만 해주면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91566618, 0.9621985 , 0.058322  ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "random_parameters = parameters.copy()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "## $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z += X[i]*parameters[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내적 함수 이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p = 1/(1+e^{-\\beta *X})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = dot_product(X, parameters)\n",
    "    p = 1/(1 + np.exp(-z))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid function로 불리며, 수식 상의 $\\beta*X$는 Z로 표현되기도 한다.  \n",
    "sigmoid function은 0에서 1 사이의 값만은 output으로 갖기 때문에 binary classification 혹은 확률 P를 나타내는 데 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8868508832739671"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "<br>\n",
    "선형 회귀의 목적함수\n",
    "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
    "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
    "  \n",
    "로지스틱 회귀의 목적함수\n",
    "## $l(p) =-\\sum \\left \\{ y_{i}logp + (1-y_{i})log(1-p)  \\right \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE를 목적함수 (=비용함수)로 사용하는 linear regression과 달리 logistic regression은 목적함수로 MSE를 사용하지 않는다. MSE를 목적함수로 Gradiant Descent 사용 시 Convex function이 아니게 되어 찾고자하는 최솟값이 아닌 다른 최솟값에 도달할 수 있기 때문이다.  \n",
    "<br>\n",
    "따라서, MSE를 대체할 수 있는 목적함수를 찾은 결과, 다음 조건을 만족한다.  \n",
    "       $if, y = 1\\rightarrow cost(p,y) = −log(p)$  \n",
    "       $if, y = 0\\rightarrow cost(p,y) = −log(1-p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X, parameters)\n",
    "    loss = (y*np.log(p)+(1-y)*np.log(1-p))\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_i(X, y, parameters):\n",
    "    y_hat = dot_product(X, parameters.T)\n",
    "    loss = np.square(np.subtract(y,y_hat))/2  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X, y, parameters)\n",
    "    loss = loss/n #loss 평균값으로 계산\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2622088980367838"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)=\\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)X_{ij}$  \n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)=-\\sum \\left ( y_{i}-p_{i} \\right )x_{ij}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_ij(X, y, parameters, j, model):\n",
    "    if model == 'linear':\n",
    "        y_hat = dot_product(X, parameters)\n",
    "        gradient = (y-y_hat)*X[j]/2\n",
    "    else:\n",
    "        p = logistic(X, parameters)\n",
    "        gradient = -(y-p)*X[j]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04943986877732968"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient\n",
    "배치 경사 하강법이란 매 경사 하강법 step에서 전체 훈련 세트 X에 대해 계산하는 기법으로, 특성 수에 민감하지 않지만 훈련 세트가 클수록 속도가 느려지는 단점이 존재한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for i in range(len(parameters))]\n",
    "    \n",
    "    for i in range(len(X_set)):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij(X, y, parameters, j, model)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59.56350891660878, 9.874540091268805, 35.34245639055461]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch\n",
    "미니 배치라 불리우는 임의의 작은 샘플 세트에 대해 그레디언트를 계산하는 기법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
    "- batch_idx 함수는 parameter로 입력받은 batch_size를 기준으로 데이터를 인덱싱하여 미니배치로 만든다음 그레디언트를 계산하는 함수이다. \n",
    "- 미니배치를 어느 정도 크게 하면 Stochastic Gradient Descent(SGD)보다 최솟값에 더 가까이 도달할 수 있다고 한다. 다만, 해당 최솟값이 local minimum일 경우 빠져나오는 데 더 힘들다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "기울기를 갱신하는 코드를 작성해주세요  \n",
    "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)):\n",
    "        gradients[i] *= learning_rate\n",
    "    \n",
    "    parameters -= gradients\n",
    "    return parameters/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00213354,  0.00575635, -0.00196735])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(parameters, gradients1, 0.01, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
    "\n",
    "- learning_rate: 학습률  \n",
    "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
    "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
    "- epoch: 한 반복에서 훈련 세트의 샘플 수만큼 반복되는 데, 이때 각 반복을 의미\n",
    "- num_epoch: 최대반복횟수 \n",
    "<br>\n",
    "\n",
    "BGD: 학습 한 번에 모든 데이터셋에 대해 기울기를 구한다.    \n",
    "SGD: 학습 한 번에 임의의 데이터에 대해서만 기울기를 구한다.   \n",
    "MGD: 학습 한 번에 데이터셋의 일부에 대해서만 기울기를 구한다.    \n",
    "\n",
    "<br>\n",
    "batch_size에 따른 경사하강법의 종류를 적어주세요. <br>      \n",
    "batch_size=1 -> SGD  <br>\n",
    "batch_size=k -> MGD  <br>\n",
    "batch_size=whole -> BGD       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.random.rand(N)\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    loss = 999\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        if stopper:\n",
    "            break\n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch, y_batch, parameters, model)\n",
    "            parameters = step(parameters, gradients, learning_rate, N)\n",
    "            new_loss = batch_loss(X_batch, y_batch, parameters, loss_function, N)\n",
    "            \n",
    "            #중단 조건\n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                break\n",
    "            loss = new_loss\n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 37.20965756174921  params: [-1.17594477 -0.37162372 -1.42731724]  gradients: [3.7036214444914677, 1.9438373659170158, 4.872363898991582]\n",
      "epoch: 100  loss: 41.114250556074424  params: [-0.78162073 -0.40970799 -1.62517229]  gradients: [1.5175081326540993, 2.925377337405446, 5.283478944677073]\n",
      "epoch: 200  loss: 41.114250556074424  params: [-0.78162073 -0.40970799 -1.62517229]  gradients: [1.5175081326540993, 2.925377337405446, 5.283478944677073]\n",
      "epoch: 300  loss: 41.114250556074424  params: [-0.78162073 -0.40970799 -1.62517229]  gradients: [1.5175081326540993, 2.925377337405446, 5.283478944677073]\n",
      "epoch: 400  loss: 41.114250556074424  params: [-0.78162073 -0.40970799 -1.62517229]  gradients: [1.5175081326540993, 2.925377337405446, 5.283478944677073]\n",
      "epoch: 500  loss: 41.114250556074424  params: [-0.78162073 -0.40970799 -1.62517229]  gradients: [1.5175081326540993, 2.925377337405446, 5.283478944677073]\n",
      "epoch: 600  loss: 41.114250556074424  params: [-0.78162073 -0.40970799 -1.62517229]  gradients: [1.5175081326540993, 2.925377337405446, 5.283478944677073]\n",
      "epoch: 700  loss: 41.114250556074424  params: [-0.78162073 -0.40970799 -1.62517229]  gradients: [1.5175081326540993, 2.925377337405446, 5.283478944677073]\n",
      "epoch: 800  loss: 41.114250556074424  params: [-0.78162073 -0.40970799 -1.62517229]  gradients: [1.5175081326540993, 2.925377337405446, 5.283478944677073]\n",
      "epoch: 900  loss: 41.114250556074424  params: [-0.78162073 -0.40970799 -1.62517229]  gradients: [1.5175081326540993, 2.925377337405446, 5.283478944677073]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.82735406,  1.69625338,  0.40796209])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 150)\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.22254266312583326  params: [-0.0238071   0.01267756  0.01305256]  gradients: [0.05034448072168998, -0.051530288191216134, -0.05756060462097561]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.0238071 ,  0.01267756,  0.01305256])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 1)\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.2227073317837822  params: [-0.17763866  0.10812902 -0.1382262 ]  gradients: [0.19178808909849174, 0.18595917534256212, 0.26110984485577965]\n",
      "epoch: 100  loss: 1.178571863811982  params: [-0.16632678  0.06529327 -0.17764872]  gradients: [0.14843344163115302, 0.13744379272266383, 0.21105211535354906]\n",
      "epoch: 200  loss: 1.178571863811982  params: [-0.16632678  0.06529327 -0.17764872]  gradients: [0.14843344163115302, 0.13744379272266383, 0.21105211535354906]\n",
      "epoch: 300  loss: 1.178571863811982  params: [-0.16632678  0.06529327 -0.17764872]  gradients: [0.14843344163115302, 0.13744379272266383, 0.21105211535354906]\n",
      "epoch: 400  loss: 1.178571863811982  params: [-0.16632678  0.06529327 -0.17764872]  gradients: [0.14843344163115302, 0.13744379272266383, 0.21105211535354906]\n",
      "epoch: 500  loss: 1.178571863811982  params: [-0.16632678  0.06529327 -0.17764872]  gradients: [0.14843344163115302, 0.13744379272266383, 0.21105211535354906]\n",
      "epoch: 600  loss: 1.178571863811982  params: [-0.16632678  0.06529327 -0.17764872]  gradients: [0.14843344163115302, 0.13744379272266383, 0.21105211535354906]\n",
      "epoch: 700  loss: 1.178571863811982  params: [-0.16632678  0.06529327 -0.17764872]  gradients: [0.14843344163115302, 0.13744379272266383, 0.21105211535354906]\n",
      "epoch: 800  loss: 1.178571863811982  params: [-0.16632678  0.06529327 -0.17764872]  gradients: [0.14843344163115302, 0.13744379272266383, 0.21105211535354906]\n",
      "epoch: 900  loss: 1.178571863811982  params: [-0.16632678  0.06529327 -0.17764872]  gradients: [0.14843344163115302, 0.13744379272266383, 0.21105211535354906]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.16632678,  0.06529327, -0.17764872])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 72)\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2. learning_rate = 0.001, num_epoch = 1000, tolerance = 0.00001\n",
    "learning_rate 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 39.109926228538164  params: [0.27906126 0.20062851 0.23618959]  gradients: [0.056656073749902634, 0.012853213858945271, 0.045055759689370915]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01619598,  0.00897059, -0.00701795])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train, learning_rate = 0.001, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 150)\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.2310164601019148  params: [-1.95901080e-04 -8.85473287e-07  6.16932353e-06]  gradients: [0.0005000037091515999, 0.00021865591293377553, 5.496843447179911e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.95901080e-04, -8.85473287e-07,  6.16932353e-06])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, learning_rate = 0.001, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 1)\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.3984170002048792  params: [ 0.01483096  0.00635273 -0.00179549]  gradients: [0.002091874387531021, 0.0018306458051498976, 0.0026765617434228806]\n",
      "epoch: 100  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 200  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 300  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 400  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 500  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 600  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 700  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 800  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 900  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.0029433 ,  0.00094763, -0.00180727])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.001, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 72)\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3. learning_rate = 0.001, num_epoch = 10000, tolerance = 0.00001\n",
    "num_epoch 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 35.40382965281298  params: [0.04504435 0.21445238 0.16288371]  gradients: [0.03753312260183264, 0.014891746369915963, 0.045275049403989284]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01619562,  0.00897234, -0.00701683])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train, learning_rate = 0.001, num_epoch = 10000, tolerance = 0.00001, model = 'logistic', batch_size = 150)\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.23101645333771778  params: [-1.95941948e-04 -8.96843797e-07  6.21709029e-06]  gradients: [0.000500003678700575, 0.000218655899617281, 5.49684311241336e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.95941948e-04, -8.96843797e-07,  6.21709029e-06])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, learning_rate = 0.001, num_epoch = 10000, tolerance = 0.00001, model = 'logistic', batch_size = 1)\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.419735995452209  params: [0.01377913 0.03572546 0.00325892]  gradients: [0.0022133510542920445, 0.001973424222884731, 0.002813518296844626]\n",
      "epoch: 100  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 200  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 300  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 400  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 500  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 600  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 700  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 800  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 900  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 1000  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 1100  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 1200  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 1300  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 1400  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 1500  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 1600  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 1700  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 1800  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 1900  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 2000  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 2100  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 2200  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 2300  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 2400  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 2500  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 2600  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 2700  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 2800  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 2900  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 3000  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 3100  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 3200  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 3300  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 3400  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 3500  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 3600  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 3700  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 3800  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 3900  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 4000  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 4100  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 4200  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 4300  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 4400  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 4500  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 4600  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 4700  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 4800  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 4900  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5000  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 5100  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 5200  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 5300  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 5400  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 5500  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 5600  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 5700  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 5800  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 5900  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 6000  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 6100  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 6200  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 6300  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 6400  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 6500  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 6600  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 6700  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 6800  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 6900  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 7000  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 7100  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 7200  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 7300  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 7400  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 7500  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 7600  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 7700  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 7800  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 7900  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 8000  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 8100  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 8200  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 8300  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 8400  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 8500  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 8600  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 8700  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 8800  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 8900  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 9000  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 9100  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 9200  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 9300  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 9400  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 9500  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 9600  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 9700  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 9800  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n",
      "epoch: 9900  loss: 1.3833247456167503  params: [-0.0029433   0.00094763 -0.00180727]  gradients: [0.0019917714333218613, 0.0017420531519985875, 0.002587666675657426]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.0029433 ,  0.00094763, -0.00180727])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.001, num_epoch = 10000, tolerance = 0.00001, model = 'logistic', batch_size = 72)\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 4. learning_rate = 0.001, num_epoch = 10000, tolerance = 0.001\n",
    "tolerance 감소  \n",
    "- 그레디언트 벡터의 norm이 허용오차(tolerance)보다 작아지면 GD가 거의 최솟값에 도달했으므로, tolerance가 감소하면 그만큼 반복횟수는 증가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 38.06035086364299  params: [0.2717144  0.03807513 0.03337958]  gradients: [0.06371172972602104, -0.012209085559385031, 0.020048401243791973]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.016164  ,  0.00897274, -0.00701429])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train, learning_rate = 0.001, num_epoch = 10000, tolerance = 0.001, model = 'logistic', batch_size = 150)\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.23106993808119988  params: [0.00096048 0.00064257 0.0001283 ]  gradients: [0.0005004335377007417, -0.0006548674299208984, 2.200627525045291e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00096048, 0.00064257, 0.0001283 ])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, learning_rate = 0.001, num_epoch = 10000, tolerance = 0.001, model = 'logistic', batch_size = 1)\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.4241673941106212  params: [0.01735983 0.02148055 0.01518791]  gradients: [0.00222231592119025, 0.001972930990221792, 0.0028252005280486473]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00683536,  0.00458756, -0.00283205])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train, learning_rate = 0.001, num_epoch = 10000, tolerance = 0.001, model = 'logistic', batch_size = 72)\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd)\n",
    "    if p> 0.5 :\n",
    "        y_predict.append(1)\n",
    "    else :\n",
    "        y_predict.append(0)\n",
    "y_predict_random = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
    "    if p> 0.5 :\n",
    "        y_predict_random.append(1)\n",
    "    else :\n",
    "        y_predict_random.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40,  0],\n",
       "       [10,  0]], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn) / (tp+fn+fp+tn)\n",
    "print(\"accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "### $y = 0.5 + 2.7x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = np.random.rand(150)\n",
    "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([1 for _ in range(150)])\n",
    "X = np.vstack((tmp, raw_X)).T\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66504227, 2.19750227])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규방정식\n",
    "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 114.05489874362357  params: [0.25057012 0.28327068]  gradients: [0.005788771365800701, 0.0037215673353519813]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01060924, -0.00641312])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#경사하강법\n",
    "new_param = gradient_descent(X, y, learning_rate = 0.00005, num_epoch = 100000, tolerance = 0.00001, model = 'linear', batch_size = 150)\n",
    "new_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_NE = theta.dot(X.T)\n",
    "y_hat_GD = new_param.dot(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
    "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
    "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
    "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
